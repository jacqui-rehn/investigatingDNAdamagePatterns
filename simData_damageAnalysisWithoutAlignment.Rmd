---
title: "Simulated datasets - Change in base proportions"
author: "Jacqueline Rehn"
date: "11/1/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#load packages
library(tibble)
library(magrittr)
library(readr)
library(reshape2)
library(stringr)
library(ggplot2)
library(gridExtra)
library(pander)
library(dplyr)

#set aesthetics for plots
theme_set(theme_bw())
palette <- c("#FF3333", "#3333FF", "#009900", "#FF9900", "#990099", 
             "#33CCCC", "#66CC66", "#FFCC66", "#FF99CC", "#3399FF", 
             "#FF6666", "#9966FF")

palette15 <- c("#FF3333", "#3333FF", "#009900", "#FF9900", "#FF99CC", 
               "#3399FF", "#66CC66", "#FFCC66", "#FF6666", "#006699",
               "#336600", "#FFCC99", "#FF0066", "#9966FF", "#33CCCC")

```


## Detecting aDNA damage

DNA is known to degrade over time. This typically results in short fragments of DNA (<100bp) and cytosine deamination to uracil at the ends of the fragments. When sequenced, the presence of uracil results in an increase in a cytosine to thymine substitutions. When these sequenced reads are aligned against a modern reference sequence this damage is observed as an increased frequency of C->T mutations at the 5' end of the read and G->A at the 3' end. These patterns of damage can be quantified using the program mapDamage2.0, however this process is dependent on ability to align aDNA sequences against a reference genome. There are two instances where this is particularly difficult:

1. The sequences have been produced from a complex metagenomic sample such as dental calculus where it is difficult to identify what microbial species. Due to sequence conservation between microbial species, aligment of the sequenced data against modern reference genomes using bwa results will result in spurious alignments, inflating/deflating damage estimates.

2. When aDNA sequences have been produced from biological material for which no modern reference currently exists and thus it is not possible to align the sequences and detect frequency of substitutions.

In these cases it would be useful to estimate the amount of damage present in the aDNA reads using a method that does not rely on alignment to a reference genome. One way this may be accomplished is by plotting the length distribution of reads directly from the fastq files and also calculating the proportions of A,T,C and G at each position along the read. If uracil was present at the ends of fragments and resulted in miscoding lesions it is expected that we would observe an increase in the proportion of T at the 5' ends of reads and an increase in the proportion of A at the 3' ends.

## Simulated Data

30 Simulated datasets were provided by Raphael Eisenhofer from ACAD which had been previously used to benchmark MALT and MALTx. Each dataset contained 1.5 million reads generated by fragmenting 29 bacterial genomes (25 common plaque bacteria and 4 common environmental/laboratory contaminant bacteria) in gargamel. The table and plot below describe the Taxons included in all datasets and their abundance.

```{r simulatedMetagenomes, message=FALSE}

#import data regarding summarising genomes used to simulate datasets
simData <- read_csv("SimulatedMetagenome.csv", col_names = FALSE, col_types = "c-n-c-cn--", skip = 1)
colnames(simData) <- c("taxon", "abundance", "genus", "contaminant", "GC")
simData <- simData %>% filter(abundance < 1)

#Present data in table
simData %>% pander(caption = "Summary of genome abundances used in all simulated datasets")

#Calculate abundance for each genome and plot as a barchart, indicating if genus origin is oral or contaminant
genusAbundance <- simData %>% select(-taxon, -contaminant, -GC) %>% group_by(genus) %>% summarise_each(funs(sum))
genusAbundance <- simData %>% select(genus, contaminant) %>% unique() %>% left_join(genusAbundance)
genusAbundance %>% 
  ggplot(aes(x=genus, y=abundance, fill=contaminant)) + 
  geom_bar(stat = "identity", colour = "white", position = position_dodge()) + 
  labs(x="", y="Abundance", title="Simulated abundance by genus") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.2, hjust = 1)) + 
  scale_fill_manual(values = c("#FF6666", "#3399FF"), 
                      name = "", 
                      breaks=c("FALSE", "TRUE"), 
                      labels=c("Oral bacteria", "Common contaminant"))
```


Datasets of various lengths and damage profiles were simulated. Fixed read lengths of 30, 50, 70 and 90bp were simulated as well as an empirical ancient DNA fragment length distribution (Empirical). For each read length profile a damaged (suffix _d.fa.gz) and undamaged (suffix .b.fa.gz) data set was simulated. For damaged sets deamination rates for single-stranded overhangs were simulated at either 10%, 50% or ~20% (Real-damage-profile). 

## Fastq counts

The following bash script was used to collate fragment lengths as well as the proportion of each base at the first 10 and last 10 positions in the sequences.

```{bash eval=FALSE}
#!/bin/bash

#Assess fragment length of reads and calculate proportion of each base at both ends of fastq reads

#Specify variables
ROOTDIR=/home/a1698312
TRIMDIR=$ROOTDIR/simData/data
LOGFILE=$ROOTDIR/simData/data/damageAnalysisLog.txt

################ Count read lengths ######################

#Change into directory where trimmed_fastq files located
if [ -d ${TRIMDIR} ]
then
  echo "Changing to trimData directory"
  cd ${TRIMDIR}
else
  echo "Cannot find ${TRIMDIR}"
exit1
fi

##### create log file for script ####

if [ ! -f damageAnalysisLog.txt ]
then
  echo -e 'Creating file damageAnalysisLog.txt'
  echo -e "file\ttime" > ${LOGFILE}
else
  echo -e 'damageAnalysisLog.txt already exists'
fi

#Generate text file for storing length count data
if [ ! -f fastq_length.txt ]
then
  echo -e 'Creating file fastq_length.txt'
  echo -e 'occ\tlength' > fastq_length.txt
else
  echo  'fastq_length.txt already exists'
fi

#Generate text file for storing base proportions
if [ ! -f new_base_proportions.txt ]
then
  echo -e 'Creating file new_base_proportions.txt'
  echo -e "fileName\tend\tpos\tA\tC\tG\tT" > new_base_proportions.txt
else
  echo  'new_base_proportions.txt already exists'
fi

#Count fastq lengths and proportion of each base for each file

for fastq_file in *fa.gz
  do
    #variable to start timing loop
    STARTTIME=$(date +%s)
    echo -e "Collating read lengths in ${fastq_file}"
    
    #extract DNA sequence from file, count length of each sequence, sort lengths and count number of unique
    LENGTH=$(zcat ${fastq_file} | sed -n '2~2p' | awk '{print length($1)}' | sort -n | uniq -c)
    
    #print fileName and collated lengths to bottom of fastq_length.txt
    echo -e "${fastq_file}"  >> fastq_length.txt
    echo -e "${LENGTH}"  >> fastq_length.txt
    
    #set counter to 1
    n=1
    
    echo -e "Collating base proportions in ${fastq_file}"    
    
    #loop which increases value of n(counter) each time up to 25
    while [ $n -le 25 ]
    do

      #extract DNA sequence from file, print base at position n in sequence, count number of each base
      COUNT5p=$(zcat ${fastq_file} | sed -n '2~2p' | awk 'length($0) > '"$(($n*2))"'' | gawk -F '' '{print $'"$n"'}' | \
      sort | uniq -c | awk '{print}' ORS='\t')
      
      echo -e "${fastq_file}\t5p\t${n}\t${COUNT5p}" >> new_base_proportions.txt

      #extract DNA sequences, reverse them (so now reading from 3' end of read), print base at position n, count number of each base
      COUNT3p=$(zcat ${fastq_file} | sed -n '2~2p' | awk 'length($0) > '"$(($n*2))"'' | rev | gawk -F '' '{print $'"$n"'}' | \
      sort | uniq -c | awk '{print}' ORS='\t')
      
      echo -e "${fastq_file}\t3p\t${n}\t${COUNT3p}" >> new_base_proportions.txt
      
      #add 1 to the counter
      let n=n+1
      
      #end inner/while loop
    done
    
    #calculate time taken to complete analysis on this file
    ENDTIME=$(date +%s)
    echo -e "${fastq_file}\t$(($ENDTIME - $STARTTIME))" >> ${LOGFILE}
    
  #end outer/for loop
  done

```

## Length Distributions

The length distribution for all simulated fastq files was initially visualised as a line graph. As most of the simulated data files contained fragments with a specified length, only the Empirical damage profiles are included below:

```{r length distributions, echo=FALSE}

############## Read length distributions ################

#make a list of fastq files
#fastqFiles <- list.files("data/", pattern = "fa.gz$", full.names = FALSE)

#Read-in text file fastq_length.txt and assign to object
fastqLength <- read.csv(file="data/fastq_length.txt", 
                        sep="", skip = 1, header = FALSE, 
                        col.names = c("occ", "length")) %>%
  mutate(fa = grepl("fa.gz$", occ), fileNo = cumsum(fa))

#extract the fileInfo and fileNo information
fastqFileInfo <- fastqLength[grep("fa.gz$", fastqLength$occ),] %>% 
  select(occ, fileNo) %>% 
  set_colnames(c("fileName", "fileNo"))

#rejoin fileInfo as a separate variable and remove NA results
fastqLength <- fastqLength %>% 
  left_join(fastqFileInfo, by = "fileNo") %>% 
  select(-fa, -fileNo) %>% 
  filter(length != "NA")

fastqLength$fileName <- gsub('.fa.gz', '', fastqLength$fileName)

#USE: df$fileName <- editFileNames(df)

editSimDataFileNames <- function(x){
  x$fileName <- gsub('.bam', '', x$fileName)
  x$fileName <- gsub('\\.b', '_undamaged', x$fileName)
  x$fileName <- gsub('_d', '_damaged', x$fileName)
  x$fileName <- gsub('-damage-no-adapters', '', x$fileName)
  x$fileName <- gsub('Real-damage-profile-', 'Real-profile', x$fileName)
  x$fileName <- gsub('no-adapters', '', x$fileName)
  x$fileName <- gsub('-damage-ACAD-adapters', '', x$fileName)
  x$fileName <- gsub('0-', '0.', x$fileName)
  x$fileName <- gsub('_noDamage_damaged', '_undamaged', x$fileName)
  x$fileName <- gsub('_withDamage', '', x$fileName)
  x$fileName <- gsub('_Real-profile_undamaged', '_undamaged', x$fileName)
}

fastqLength$fileName <- editSimDataFileNames(fastqLength)

#Convert fastqLength variable from factor to numeric
fastqLength <- fastqLength %>% mutate_if(is.factor, as.character)
fastqLength$occ <- as.numeric(fastqLength$occ)

#Plot the length distributions taken from the fastq files
fastqLength %>% subset(grepl("Empirical", fileName)) %>%
  ggplot(aes(x=length, y=occ, colour=fileName)) + 
  geom_line() + 
  labs(x="Read length", y="Number of reads", 
       colour="Sample", title="Length distribution of fastq reads")  
  #scale_colour_manual(values = palette)
```

We observe only a single line, indicating that the length distribution of all 6 simulated files is identical. This can be confirmed by re-plotting the same data using facet_wrap.

```{r echo=FALSE}

fastqLength %>% subset(grepl("Empirical", fileName)) %>%
  ggplot(aes(x=length, y=occ, colour=fileName)) + 
  geom_line() + 
  labs(x="Read length", y="Number of reads", title="Length distribution of fastq reads") + 
  scale_colour_manual(values = palette15) + 
  facet_wrap(~fileName) +
  guides(colour=FALSE)

```

## Box plot comparing length distributions for each fastq file

ggplot2 requires data for boxplots to be in the form of a vector of individual lengths. This can be completed using ```r, rep(x$length,x$occ)```. However, for large fastq files this requires too much memory (if 50 million reads this will produce a vector of length 50 million). Instead a function was written to scale the frequency data prior to converting to a vector. This conserves memory and still enables a box-plot to be draw which is representative of the length distributions in the file. As there are a large number of simulated data sets, only those simulated with the Real-damage-profile have been plotted for ease of comparison.

```{r echo=FALSE}

#Write a function to scale the occ by factor of 100 and use scaled value to rep the lengths
scale_lengths <- function(x){
  x <- x %>% split(f = .$fileName) %>% 
    lapply(function(x){
      data_frame(length = rep(x$length,round((x$occ)/1000)))
    }) %>% bind_rows(.id = "fileName")
  return(x)
}

#apply scale_lengths() function to each of the samples
expandedLengths <- scale_lengths(fastqLength)

#Bind each of the scaled lengths vectors into one data_frame and generate boxplot
filter(expandedLengths, grepl("Real-profile", fileName)) %>% 
  ggplot(aes(x=fileName, y=length, fill=fileName)) + 
  geom_boxplot(outlier.color = "dark grey", outlier.size = 0.3) + 
  theme(axis.title.x = element_blank(), axis.text.x = element_text(angle = 90, vjust = 0.2, hjust = 1)) + 
  ylab("Fragment Length") + 
  guides(fill=FALSE) +
  ggtitle("Fastq read lengths by sample")

```

As indicated, data-sets were simulated to contain fixed fragment lengths of 30,50,70 or 90bp (using -l <int> option in gargammel) or empirical fragment lengths representative of aDNA data (using --loc 4 --scale 3 option). The Empirical simulations demonstrate identical length distributions regardless of whether the reads were subsequently damaged or not. 

## Summary statistics for length distributions

The number of reads, mean read length and standard deviation for lengths is computed from the frequency data for all fastq files and displayed in the following table. Function written for calculating these stats as converting frequency data to a vector for large fastq files requires too much memory.

```{r echo=FALSE}

##### Create a table summarising key statistics (No.Reads, mean length, standard deviation) ####

#Mutate fastqLength to include count, length.occ, mean, (length-mean)^2*occ
fastqLength <- fastqLength %>% group_by(fileName) %>% 
  mutate(count = sum(occ), length.occ = length*occ, mean = sum(length.occ)/count, start.var = ((length-mean)^2)*occ)

#find count, mean and sd for length.df
length.stats <- fastqLength %>% group_by(fileName) %>% 
  summarise(count = sum(occ), mean = sum(length.occ)/count, sd = sqrt(sum(start.var)/count))

panderOptions("table.split.table", Inf)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', TRUE)
length.stats %>% pandoc.table(caption = "Summary statistics for fastq read lengths", justify = "right")

```

## Nucleotide substitution rate

Data sets with fixed fragment lengths were simulated to contain single-stranded overhang cytosine deamination rates of 10% or 50%. Files with a .b suffix are undamaged while those with a _d are damaged. Files indicating 'Real-damage-profile' have been simulated by specifying mapDamage parameter settings of .... resulting in ~20% single-stranded overhang deamination.

The number of each base present in each position for the first and last 25bp of each read was counted and stored in the file new_base_proportions.txt. This data was imported into R and plotted with the following function.

```{r base proportions, message=FALSE, warning=FALSE}

#read-in text file base_proportions.txt and assign to object
newBaseProp <- read_delim(file = "data/new_base_proportions.txt", delim = "\t", skip = 1, 
                          col_names = c("fileName", "end", "pos", "A", "C", "G", "T"), 
                          col_types = "ccncccc-", na = c(" "))
#remove unnecessary bases listed
newBaseProp$A <- gsub(' A', '', newBaseProp$A)
newBaseProp$C <- gsub(' C', '', newBaseProp$C)
newBaseProp$G <- gsub(' G', '', newBaseProp$G)
newBaseProp$T <- gsub(' T', '', newBaseProp$T)
#convert counts for each base from character to numeric
newBaseProp[4:7] <- lapply(newBaseProp[4:7], as.numeric)

#remove rows with NA
newBaseProp <- na.omit(newBaseProp)

#edit fileName to remove unnecessary information
newBaseProp$fileName <- gsub('.fa.gz', '', newBaseProp$fileName)
newBaseProp$fileName <- editSimDataFileNames(newBaseProp)

########### Calculate base proportions ############

newBaseProp <- newBaseProp %>% 
  mutate(total = (A + C + G + T)) %>% 
  mutate(T = T/total, G = G/total, C = C/total, A = A/total) %>% 
  select(-total) 

######### Visualise base proportions ##############

# create graphing function
baseProp.graph <- function(df, na.rm = TRUE, ...){
  
  # create list of sampleID's in data to loop over
  fileName_list <- unique(df$fileName)
  
  # create for loop to split data based on sampleID 
  for (i in seq_along(fileName_list)) {
    
    # create object to store 5p data
    baseProp_5p <- subset(df, df$fileName==fileName_list[i]) %>% 
      filter(end == "5p") %>% melt(id.vars = c("fileName", "end", "pos"))
    
    # create object to store 3p data
    baseProp_3p <- subset(df, df$fileName==fileName_list[i]) %>% 
      filter(end == "3p") %>% melt(id.vars = c("fileName", "end", "pos"))
    
    # plot to object, 5p data
    plotBaseProp5p <- baseProp_5p %>% 
      ggplot(aes(x=pos, y=value, colour=variable)) + 
      geom_line() +
      theme_bw() +
      ylim(0.1,0.4) +
      scale_colour_manual(values = c("#FF9900","#3333FF","#009900","#FF3333"), name="Base", breaks=c("T","C","A","G")) + 
      theme(legend.position="none") + 
      labs(x="Position from 5' end of read", y="Proportion of bases")
    
    # plot to object, 3p data   
    plotBaseProp3p <- baseProp_3p %>% 
      ggplot(aes(x=pos, y=value, colour=variable)) + 
      geom_line() +
      theme_bw() +
      scale_y_continuous(limits = c(0.1,0.4), position = "right") +
      scale_x_reverse() +
      scale_colour_manual(values = c("#FF9900","#3333FF","#009900","#FF3333"), name="Base", breaks=c("T","C","A","G")) + 
      labs(x="Position from 3' end of read", y="")
    
    #print plots
    grid.arrange(plotBaseProp5p, plotBaseProp3p, 
                 ncol = 2, 
                 widths=c(0.85,1), 
                 top = (paste(fileName_list[i])))
    
    #End loop
  }
}

```

As there are 30 data-sets it is easier to plot these in groups. Initially all undamaged datasets with a fixed length were plotted.

```{r undamaged with fixed length, message=FALSE}

# run graphing function on results from all undamaged datasets with fixed proportions of microbes
filter(newBaseProp, grepl("undamaged", fileName)) %>% baseProp.graph()

# calculate expected GC and AT content for simulated data sets
expectedGC <- simData %>% select(abundance, GC) %>% 
  mutate(contributingGC = abundance*GC) %>% 
  summarise(overallGC = sum(contributingGC))
expectedAT <- 100 - expectedGC

```

We see in all cases that the proportion of A=T and the proportion of G=C. There is a higher proportion of A/T than G/C, indicating that the genomes used in these simulated data sets have a combined GC content of ~44%. This can be confirmed by calculating the expected GC content of the overall data using the GC content of each genome included multiplied by its %Abundance.

It is important to note that these proporitons would vary depending on the overall GC content of the sample. To confirm that these proportions fit with the simulated data the GC content of each taxon was multiplied by its abundance and the results summed together to give an expected GC content of `r expectedGC`%. Thus the AT content is expected to be `r expectedAT`%.

Damaged datasets for each length profile but with 10% simulated damage are shown below:

```{r message=FALSE}

# run graphing function on results from all data sets with a simulated damage level of 10%
filter(newBaseProp, grepl("_0.1_damaged", fileName)) %>% baseProp.graph()

```

In each of these instances the 5' ends of the reads demonstrate a slight increase in proportion of T and a decrease in the proportion of C, as we would expect if miscoding lesions have occured resulting in a C->T substitution. The exact change in proporiton of bases between the first and tenth positions is summarised in the table.

```{r message=FALSE}

#write above into a function that produces a single data frame from which required results can be extracted.

Calculate_BasePropChange <- function(df, ...){
  
  #extract data for final position in each file
  #maxPos <- df %>% split(f = .$end) %>% 
   # lapply(function(x){x[cumsum(rle(x$fileName)$lengths), ]}) %>% 
   # bind_rows() %>% 
   # melt(id.vars = c("fileName", "end", "pos"))
  
  #extract data for 10th position in each file
  maxPos <- df %>% split(f = .$end) %>% 
    lapply(function(x){x %>% filter(pos == "10")}) %>% 
    bind_rows() %>% 
    melt(id.vars = c("fileName", "end", "pos"))
  
  #extract information for position 1 and join to above
  minPos <- df %>% split(f = .$end) %>% 
    lapply(function(x){x %>% filter(pos == "1")}) %>% 
    bind_rows() %>% 
    melt(id.vars = c("fileName", "end", "pos"))
  
  #calculate the change in base proportion
  change_in_prop <- left_join(minPos, maxPos, by = c("fileName", "end", "variable")) %>% 
    mutate(change = (value.x - value.y)*100) %>% 
    select(fileName, end, variable, change)
  
  #convert from long to wide format
  change_in_prop <- dcast(change_in_prop, fileName + end ~ variable, value.var = "change")
    
  #round values to 4 decimal places
  change_in_prop <- change_in_prop %>% mutate_each(funs(round(.,2)), T,A,C,G)
  
  #return change_in_prop
  return(change_in_prop)
}

#view table
filter(newBaseProp, grepl("_0.1_damaged", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions")

```

There has been a `r filter(newBaseProp, grepl("_0-1.+_d", fileName)) %>% Calculate_BasePropChange() %>% filter(end == "5p") %>% summarise(mean(.$T))` increase in the proportion of T bases between position 10 and position 1 of the 5p' end of reads and a similar decrease in the proportion of C bases across the same region, indicative of C -> T miscoding lesions. At the 3p' end the same pattern is observed except we see G -> A mutations as a result of the library preparation process. 

Thus a 10% deamination rate appears to result in ~1.4% change in proportion of bases, regardless of strand length. It would therefore be expected that a 50% deamination rate would result in an ~7% change in proportion of bases at the ends of reads.

```{r message=FALSE}

# run graphing function on results from all damaged datasets with a deamination rate of 50%
filter(newBaseProp, grepl("_0.5_damaged", fileName)) %>% baseProp.graph()

```

As expected there has been a change in the proportion of bases at the ends of the reads of ~7.6% with C -> T at 5' end and G -> A at 3' end. 

```{r message=FALSE}

#generate table displaying calculated change in base proportions for 50% damage
filter(newBaseProp, grepl("_0.5_damaged", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions")

```

The final data sets were simulated using an empirically determine deamination rate of ~20% based on samples held at ACAD. This is predicted to result in an ~3% change in the proportion of C/T at 5' end and G/A at 3' end.

```{r message=FALSE}
#plot graphs on real-damage-profile data sets
filter(newBaseProp, grepl("_Real-profile_damaged", fileName)) %>% baseProp.graph()

```

In these profiles the damage is restricted to the ends (last 5-6 bp), rather than expanding further along the read, even for data set with longer simulated lengths. There has been a ~3% change in the proportion of bases at the ends of the reads as predicted, indicative of ~20% deamination rate. 

```{r message=FALSE}

#generate table displaying calculated change in base proportions for 50% damage
filter(newBaseProp, grepl("_Real-profile_damaged", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions")

```

### What effect does inclusion of contamination have on base proportions

These simulated data sets demonstrate that the base proportion at the ends of the reads can be used to indicate the level of cytosine deamination when the extent of damage is consistent across all reads in the sample. This however does not reflect what commonly occurs in ancient samples. Most ancient samples contain a high proportion of contaminant sequences either from the environment in which the original dental calculus material was preserved (soil) and stored (museum), as well as contaminant sequences from the laboratory in which DNA extraction and sequencing took place. These contaminant sequences contain levels of DNA damage that can vary from that observed in microbial DNA that is endogenous to the dental calculus. In a study by Philips et al (2017), evaluation of the DNA damage present in environmental, human related and oral sequences extracted from teeth of 161 archeaological remains revealed that environmental sequences typically demonstrate lower levels of dmaage than human related or oral microbial sequences, consistent with the hypothesis that these environmental bacterial sequences invaded the remains post-humously and therefore are younger than the endogenous sequences. Likewise it would be expected that bacterial sequences entering the sequencing data due to laboratory contaiminant would be modern in origin and thus demonstrate little to no damage patterns. Consequently, the overall level of cytosine deamination in the sequenced data would vary depending on the level of environment and laboratory contamination. 

To observe the effects of this additional data sets were simulated from the same 29 microbial genomes, but in this instance 2 genomes that are common laboratory contaminants and 2 genomes that are common soil microbiota were used to represent varying levels of environmental and laboratory contamination. These respective genomes were concatenated and placed into the /endo and /cont subdirectories of input files for gargammel. New data sets were then simulated using the following lengths and levels of contamination:

% enviromental | % lab contaminant | % endogenous microbes
---------------|-------------------|-----------------------
 0.10          | 0.05              | 0.85   
 0.35          | 0.05              | 0.60    
 0.60          | 0.05              | 0.35
 0.85          | 0.05             | 0.10



Plot undamaged file to see new GC content

```{r message=FALSE}

filter(newBaseProp, grepl("endo.+_undamaged", fileName)) %>% baseProp.graph()

```

Here we can see that without any simulated damage the proportion of bases remains constant across the ends of the reads. The GC content varies between files due to variation in the proportion of contaminant environmental sequences. These genomes (A.tumefaciens and B.subtilis) have GC contents of 59.2% and 43.6% respectively. When these genomes contribute to only 6% of the simulated reads (as with all previous data sets) the GC content of the file is ~44%. When this proportion increases (to 10, 35, 60 and finally 85% of the reads) the GC content increases to ~51%.


```{r message=FALSE}
#plot graphs on lowCont data sets
filter(newBaseProp, grepl("_endo-.+_damaged", fileName)) %>% baseProp.graph()

```

```{r message=FALSE}
#generate table displaying calculated change in base proportions for lowCont files
filter(newBaseProp, grepl("_endo-0.85_.+_damaged", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions for low contaminant samples")
```

With low-moderate contamination

```{r message=FALSE}
#plot graphs on lowCont data sets
filter(newBaseProp, grepl("_endo-0.6_.+_damaged", fileName)) %>% baseProp.graph()

#generate table displaying calculated change in base proportions for low-modCont files
filter(newBaseProp, grepl("_endo-0.6_.+_damaged", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions for low-moderately contaminant samples")

```

With moderate levels of contamination

```{r message=FALSE}
#plot graphs on lowCont data sets
filter(newBaseProp, grepl("_endo-0.35.+_damaged", fileName)) %>% baseProp.graph()

#generate table displaying calculated change in base proportions for low-modCont files
filter(newBaseProp, grepl("_endo-0.35_.+_damaged", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions for moderately contaminant samples")

```

with high levels of contamination

```{r message=FALSE}

#plot graphs on highCont data sets
filter(newBaseProp, grepl("_endo-0.1.+_damaged", fileName)) %>% baseProp.graph()

#generate table displaying calculated change in base proportions for low-modCont files
filter(newBaseProp, grepl("_endo-0.1_.+_damaged", fileName)) %>% 
  Calculate_BasePropChange() %>% 
  pander("Percentage change in base proportions for highly contaminant samples")

```

## Session Info

```{r}
sessionInfo()
```

